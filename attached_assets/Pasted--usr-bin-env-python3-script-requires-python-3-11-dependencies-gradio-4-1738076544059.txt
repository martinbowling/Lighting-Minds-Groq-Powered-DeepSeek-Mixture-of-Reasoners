#!/usr/bin/env python3
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "gradio>=4.19.2",
#     "groq>=0.5.0",
#     "python-dotenv>=1.0.0",
# ]
# ///

import gradio as gr
import asyncio
import os
from groq import Groq
from dotenv import load_dotenv
import time

load_dotenv()

ENV_GROQ_KEY = os.getenv("GROQ_API_KEY", "")
reasoners_history = []

async def get_reasoners(message, groq_key):
    """Get reasoners with emojis from LLM using llama-3.3-70b-specdec"""
    print(f"DEBUG: Starting get_reasoners for {message}")
    client = Groq(api_key=groq_key)
    prompt = f"""For this question: "{message}", determine 3-4 essential analytical perspectives.
Return only the list in format: "EMOJI PerspectiveName, EMOJI PerspectiveName"
Example: "üî¨ Technical Analyst, ‚öñÔ∏è Legal Expert, üåç Environmental Scientist"
Focus on perspectives that will provide unique and valuable insights."""

    raw_text = ""
    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-specdec",  # Using llama for perspective identification
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=150,
            stream=True,
            timeout=30  # Add timeout as desired
        )
        print(f"DEBUG: Got response stream for get_reasoners")
        
        chunk_count = 0
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                print(f"DEBUG: Got chunk {chunk_count} for get_reasoners")
                raw_text += chunk.choices[0].delta.content
                # Stream out partial reasoner text
                yield {"type": "reasoner_list", "content": raw_text}
                
        if not raw_text:
            print(f"WARNING: No content received for get_reasoners")
            yield {"type": "reasoner_list", "content": "Reasoners failed - no content received"}
            yield {"type": "final_reasoners", "content": []}
            return
            
        print(f"DEBUG: Finished get_reasoners ({chunk_count} chunks)")
        
        # Parse the reasoners
        entries = [e.strip() for e in raw_text.split(',')]
        parsed = []
        for entry in entries:
            if ' ' in entry:
                emoji = entry[0]
                name = entry[1:].strip()
                parsed.append({"emoji": emoji, "name": name})
            else:
                parsed.append({"emoji": "üß†", "name": entry})
        
        # Send final list
        yield {"type": "final_reasoners", "content": parsed[:4]}
        
    except Exception as e:
        print(f"ERROR in get_reasoners: {str(e)}")
        yield {"type": "reasoner_list", "content": f"Reasoners failed - {str(e)}"}
        yield {"type": "final_reasoners", "content": []}

async def get_reasoned_response(client, message, reasoner):
    """Get streaming response from a specific reasoner using deepseek"""
    print(f"DEBUG: Starting analysis for {reasoner['emoji']} {reasoner['name']}")
    prompt = f"""As {reasoner['emoji']} {reasoner['name']}, analyze:
{message}

Provide your analysis in this structure:
1. Core principles and methodology
2. Key observations
3. Critical implications
4. Potential limitations

Maintain focus on your specific domain expertise while acknowledging interconnections."""
    
    try:
        print(f"DEBUG: Creating completion request for {reasoner['emoji']} {reasoner['name']}")
        response = client.chat.completions.create(
            model="deepseek-r1-distill-llama-70b",  # Using deepseek for reasoning
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4096,
            stream=True,
            timeout=30  # Add timeout as desired
        )
        print(f"DEBUG: Got response stream for {reasoner['emoji']} {reasoner['name']}")
        
        analysis = ""
        chunk_count = 0
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                content = chunk.choices[0].delta.content
                print(f"DEBUG: Got chunk {chunk_count} for {reasoner['emoji']} {reasoner['name']}: {content!r}")
                analysis += content
                # Yield partial analysis so far
                yield {"type": "analysis", "reasoner": reasoner, "content": analysis}
        
        if not analysis:
            print(f"WARNING: No content received for {reasoner['emoji']} {reasoner['name']}")
            yield {"type": "analysis", "reasoner": reasoner, "content": "Analysis failed - no content received"}
            return
            
        print(f"DEBUG: Finished analysis for {reasoner['emoji']} {reasoner['name']} ({chunk_count} chunks)")
    except Exception as e:
        print(f"ERROR in get_reasoned_response for {reasoner['emoji']} {reasoner['name']}: {str(e)}")
        yield {"type": "analysis", "reasoner": reasoner, "content": f"Analysis failed - {str(e)}"}

async def synthesize_responses(client, message, responses):
    """Create integrated analysis using deepseek"""
    print("DEBUG: Starting synthesize_responses")
    
    formatted_responses = "\n\n".join([
        f"{r['reasoner']['emoji']} {r['reasoner']['name']}:\n{r['analysis']}"
        for r in responses
    ])
    
    prompt = f"""Given these expert analyses on the question "{message}":

{formatted_responses}

Create a balanced synthesis that:
1. Highlights key agreements and tensions
2. Identifies practical implications
3. Notes important considerations and tradeoffs

Focus on actionable insights while acknowledging complexity."""
    
    try:
        print("DEBUG: Creating synthesis completion request")
        response = client.chat.completions.create(
            model="deepseek-r1-distill-llama-70b",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2048,
            stream=True,
            timeout=30
        )
        print("DEBUG: Got synthesis response stream")
        
        synthesis = ""
        chunk_count = 0
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                chunk_count += 1
                content = chunk.choices[0].delta.content
                print(f"DEBUG: Got synthesis chunk {chunk_count}: {content!r}")
                synthesis += content
                yield {"type": "synthesis", "content": synthesis}
        
        if not synthesis:
            print("WARNING: No synthesis content received")
            yield {"type": "synthesis", "content": "Synthesis failed - no content received"}
            return
            
        print(f"DEBUG: Finished synthesis ({chunk_count} chunks)")
    except Exception as e:
        print(f"ERROR in synthesize_responses: {str(e)}")
        yield {"type": "synthesis", "content": f"Synthesis failed - {str(e)}"}

async def process_query(message, groq_key):
    """Full processing pipeline with streaming responses"""
    print("DEBUG: Starting process_query")
    client = Groq(api_key=groq_key)
    
    try:
        # Get reasoners
        print("DEBUG: Getting reasoners")
        yield {"type": "status", "content": "üîç Identifying perspectives..."}
        reasoners = []
        async for reasoner_update in get_reasoners(message, groq_key):
            yield reasoner_update
            if reasoner_update["type"] == "final_reasoners":
                reasoners = reasoner_update["content"]
                print(f"DEBUG: Got {len(reasoners)} reasoners")
        
        if not reasoners:
            print("DEBUG: No reasoners found")
            yield {"type": "status", "content": "‚ùå No perspectives identified. Please try again."}
            return
        
        # Process each perspective
        responses = []
        for reasoner in reasoners:
            print(f"DEBUG: Processing {reasoner['emoji']} {reasoner['name']}")
            yield {"type": "status", "content": f"üí≠ Getting {reasoner['emoji']} {reasoner['name']}'s analysis..."}
            
            analysis_content = ""
            try:
                async for analysis in get_reasoned_response(client, message, reasoner):
                    if analysis["type"] == "analysis":
                        analysis_content = analysis["content"]
                        yield analysis
                print(f"DEBUG: Got analysis for {reasoner['emoji']} {reasoner['name']}")
                
                responses.append({
                    "reasoner": reasoner,
                    "analysis": analysis_content
                })
            except Exception as e:
                print(f"ERROR processing {reasoner['emoji']} {reasoner['name']}: {str(e)}")
                continue
        
        if not responses:
            print("DEBUG: No responses collected")
            yield {"type": "status", "content": "‚ùå Failed to get analyses. Please try again."}
            return
        
        # Generate synthesis
        print("DEBUG: Starting synthesis")
        yield {"type": "status", "content": "üåü Creating integrated synthesis..."}
        async for synthesis in synthesize_responses(client, message, responses):
            yield synthesis
        print("DEBUG: Finished synthesis")
        
    except Exception as e:
        print(f"ERROR in process_query: {str(e)}")

async def chat_fn(message, history, groq_key):
    """Main chat interface with streaming updates"""
    print("DEBUG: Starting chat_fn")
    active_groq_key = ENV_GROQ_KEY or groq_key
    history = history or []
    
    if not active_groq_key:
        history.append({"role": "assistant", "content": "üîë Please configure your Groq API key!"})
        yield "", history
        return
    
    try:
        # Add user message to history
        history.append({"role": "user", "content": message})
        yield "", history
        
        current_reasoner = None
        
        async for update in process_query(message, active_groq_key):
            if update["type"] == "status":
                # Add new status message
                history.append({"role": "assistant", "content": update["content"]})
                current_reasoner = None
            
            elif update["type"] == "reasoner_list":
                # Continuously update reasoner list
                if len(history) == 0 or history[-1]["role"] != "assistant":
                    history.append({"role": "assistant", "content": ""})
                history[-1]["content"] = f"Let's hear from...\n{update['content']}"
            
            elif update["type"] == "analysis":
                # Handle streaming reasoner analysis
                content = f"### {update['reasoner']['emoji']} {update['reasoner']['name']}'s Analysis\n\n{update['content']}"
                
                # Check if we're starting a new reasoner
                if current_reasoner != update['reasoner']['name']:
                    history.append({"role": "assistant", "content": content})
                    current_reasoner = update['reasoner']['name']
                else:
                    # Update existing reasoner's content
                    history[-1]["content"] = content
            
            elif update["type"] == "synthesis":
                # Handle streaming synthesis
                content = f"# üåü Integrated Synthesis\n\n{update['content']}"
                if (
                    len(history) == 0 
                    or history[-1]["role"] != "assistant" 
                    or current_reasoner is not None
                ):
                    history.append({"role": "assistant", "content": content})
                    current_reasoner = None
                else:
                    history[-1]["content"] = content
            
            # Yield the updated history so the UI sees partial updates
            yield "", history
            
    except Exception as e:
        print(f"ERROR in chat_fn: {str(e)}")
        import traceback
        error = f"üö® Error: {str(e)}\n```\n{traceback.format_exc()}\n```"
        history.append({"role": "assistant", "content": error})
        yield "", history

def create_demo():
    print("DEBUG: Starting create_demo")
    with gr.Blocks() as demo:
        gr.Markdown("""# üß† Lighting Minds
        Explore questions through multiple analytical perspectives""")
        
        chatbot = gr.Chatbot(
            [],
            elem_id="chatbot",
            bubble_full_width=False,
            height=600,
            show_copy_button=True,
            type="messages"  # Ensures it displays roles + messages properly
        )
        
        msg = gr.Textbox(
            show_label=False,
            placeholder="Enter your question...",
            container=False
        )
        
        with gr.Row():
            submit = gr.Button("Submit")
            clear = gr.Button("Clear")
        
        gr.Markdown("""
        Powered by:
        - ü¶ô Llama 3.3 70B for perspective identification
        - üîç Deepseek R1 70B for analysis and synthesis
        """)
        
        active_groq_key = gr.State(ENV_GROQ_KEY)
        
        msg.submit(chat_fn, [msg, chatbot, active_groq_key], [msg, chatbot])
        submit.click(chat_fn, [msg, chatbot, active_groq_key], [msg, chatbot])
        clear.click(lambda: (None, None), None, [msg, chatbot])
    
    print("DEBUG: Finished create_demo")
    return demo

def main():
    # Create a .env if it does not exist
    if not os.path.exists(".env"):
        with open(".env", "w") as f:
            f.write("GROQ_API_KEY=\n")
    
    demo = create_demo()
    # The .queue() call ensures asynchronous operation in Gradio
    # You can add `every=1` or `live=True` if needed.
    demo.queue().launch()

if __name__ == "__main__":
    main()
